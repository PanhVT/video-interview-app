import os
import tempfile
import subprocess

# Lazy import ƒë·ªÉ kh√¥ng crash n·∫øu module ch∆∞a c√†i
try:
    import whisper
    WHISPER_AVAILABLE = True
except ImportError:
    WHISPER_AVAILABLE = False
    print("‚ö†Ô∏è  whisper package not installed. Local transcription will be disabled.")
    print("   Install with: pip install openai-whisper")

import io

def get_whisper_model(model_size: str = "medium"):
    """
    Load Whisper model (ch·ªâ load 1 l·∫ßn, cache l·∫°i)
    model_size: "tiny", "base", "small", "medium", "large"
    - tiny: ~39M params, fastest, lowest quality
    - base: ~74M params, good balance
    - small: ~244M params, better quality
    - medium: ~769M params, high quality (recommended for best accuracy)
    - large: ~1550M params, best quality, slowest
    """
    if not WHISPER_AVAILABLE:
        return None
    
    # Cache model ƒë·ªÉ kh√¥ng load l·∫°i m·ªói l·∫ßn
    if not hasattr(get_whisper_model, '_model_cache'):
        get_whisper_model._model_cache = {}
    
    if model_size not in get_whisper_model._model_cache:
        try:
            print(f"üì• Loading Whisper model: {model_size} (first time only, may take a moment)...")
            model = whisper.load_model(model_size)
            get_whisper_model._model_cache[model_size] = model
            print(f"‚úÖ Whisper model {model_size} loaded successfully!")
        except Exception as e:
            print(f"‚ö†Ô∏è  Error loading Whisper model: {e}")
            return None
    
    return get_whisper_model._model_cache[model_size]

def extract_audio_from_video(video_path: str, output_audio_path: str) -> bool:
    """
    Extract audio t·ª´ video file s·ª≠ d·ª•ng ffmpeg
    Returns True n·∫øu th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
    """
    try:
        # Whisper h·ªó tr·ª£ nhi·ªÅu format, d√πng MP3 cho ƒë∆°n gi·∫£n
        cmd = [
            'ffmpeg',
            '-i', video_path,
            '-vn',  # No video
            '-acodec', 'mp3',  # MP3 codec
            '-ar', '16000',  # Sample rate 16kHz (Whisper recommended)
            '-ac', '1',  # Mono channel
            '-y',  # Overwrite output file
            output_audio_path
        ]
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode == 0:
            return True
        else:
            print(f"‚ö†Ô∏è  FFmpeg error: {result.stderr}")
            return False
    except FileNotFoundError:
        print("‚ö†Ô∏è  FFmpeg not found. Please install FFmpeg and add it to PATH.")
        return False
    except Exception as e:
        print(f"‚ö†Ô∏è  Error extracting audio: {e}")
        return False

def transcribe_video(video_bytes: bytes, language: str = "en", model_size: str = "medium", translate_to_english: bool = False) -> dict:
    """
    Transcribe video s·ª≠ d·ª•ng Whisper local (MI·ªÑN PH√ç!)
    
    Args:
        video_bytes: Video file bytes (WebM format)
        language: Language code (default: "vi" for Vietnamese)
                  C√≥ th·ªÉ l√†: "vi", "en", "ja", "ko", "zh", etc.
                  Ho·∫∑c None ƒë·ªÉ auto-detect
        model_size: Model size - "tiny", "base", "small", "medium", "large"
                    Default: "base" (good balance)
    
    Returns:
        dict v·ªõi keys: 'success', 'transcript', 'confidence', 'error'
    """
    if not WHISPER_AVAILABLE:
        return {
            'success': False,
            'transcript': '',
            'confidence': 0.0,
            'error': 'whisper package not installed. Install with: pip install openai-whisper'
        }
    
    # T·∫°o temporary files
    temp_video = None
    temp_audio = None
    
    try:
        # Load model
        model = get_whisper_model(model_size)
        if not model:
            return {
                'success': False,
                'transcript': '',
                'confidence': 0.0,
                'error': 'Failed to load Whisper model'
            }
        
        # L∆∞u video v√†o temp file
        with tempfile.NamedTemporaryFile(delete=False, suffix='.webm') as f:
            f.write(video_bytes)
            temp_video = f.name
        
        # Extract audio t·ª´ video
        temp_audio = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3').name
        if not extract_audio_from_video(temp_video, temp_audio):
            return {
                'success': False,
                'transcript': '',
                'confidence': 0.0,
                'error': 'Failed to extract audio from video. Make sure ffmpeg is installed and in PATH.'
            }
        
        # Transcribe b·∫±ng Whisper
        task = "translate" if translate_to_english else "transcribe"
        task_text = "Translating to English" if translate_to_english else "Transcribing"
        print(f"üîÑ {task_text} with Whisper {model_size}...")
        
        result = model.transcribe(
            temp_audio,
            language=language if (language and not translate_to_english) else None,  # None = auto-detect, ho·∫∑c b·ªè qua n·∫øu translate
            task=task,  # "transcribe" ho·∫∑c "translate" (translate = translate to English)
            verbose=False,  # Kh√¥ng in progress
            fp16=False,  # D√πng float32 ƒë·ªÉ t∆∞∆°ng th√≠ch t·ªët h∆°n (CPU)
            condition_on_previous_text=True,  # C·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c v·ªõi context
            initial_prompt=None,  # C√≥ th·ªÉ th√™m prompt ƒë·ªÉ c·∫£i thi·ªán
            word_timestamps=False,  # Kh√¥ng c·∫ßn word timestamps ƒë·ªÉ nhanh h∆°n
            temperature=0.0  # Deterministic output, t·ªët h∆°n cho transcription
        )
        
        transcript_text = result["text"].strip()
        
        # Whisper kh√¥ng c√≥ confidence score tr·ª±c ti·∫øp
        # T√≠nh average logprob t·ª´ T·∫§T C·∫¢ segments (kh√¥ng ch·ªâ segment ƒë·∫ßu)
        segments = result.get("segments", [])
        if segments:
            # L·∫•y average logprob t·ª´ t·∫•t c·∫£ segments
            logprobs = [seg.get("avg_logprob", -1.0) for seg in segments if "avg_logprob" in seg]
            if logprobs:
                avg_logprob = sum(logprobs) / len(logprobs)
                # Convert logprob to approximate confidence (0-1 scale)
                # logprob th∆∞·ªùng t·ª´ -1.0 (k√©m) ƒë·∫øn 0.0 (t·ªët)
                # Normalize: (-1.0 -> 0.0), (-0.5 -> 0.5), (0.0 -> 1.0)
                confidence = min(1.0, max(0.0, (avg_logprob + 1.0)))
            else:
                confidence = 0.85  # Default n·∫øu kh√¥ng c√≥ logprob
        else:
            confidence = 0.85  # Default n·∫øu kh√¥ng c√≥ segments
        
        detected_language = result.get("language", language)
        
        return {
            'success': True,
            'transcript': transcript_text,
            'confidence': confidence,
            'error': None,
            'language': detected_language,
            'model': model_size
        }
        
    except Exception as e:
        error_msg = str(e)
        return {
            'success': False,
            'transcript': '',
            'confidence': 0.0,
            'error': f'Transcription error: {error_msg}'
        }
    finally:
        # Cleanup temp files
        if temp_video and os.path.exists(temp_video):
            try:
                os.unlink(temp_video)
            except:
                pass
        if temp_audio and os.path.exists(temp_audio):
            try:
                os.unlink(temp_audio)
            except:
                pass

